{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b2fa3cf-99af-4cbb-a7cf-5a98a3b7716e",
   "metadata": {
    "id": "6b2fa3cf-99af-4cbb-a7cf-5a98a3b7716e"
   },
   "source": [
    "# An introduction to Artificial Neural Networks\n",
    "Maciej Aleksandrowicz, MVG Group 2023, Machine Learning Laboratory, Version 4\n",
    "\n",
    "## Glossary\n",
    "| Acronym/Short name | Full name |\n",
    "| --- | --- |\n",
    "| ANN | Artificial Neural Network |\n",
    "| Backprog | Backward propagation |\n",
    "\n",
    "## Import packages\n",
    "For today's classes you have to use only numpy! \n",
    "\n",
    "No `from framework import solution` is allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c9aed7-9a7a-4b2f-89f8-8ab76e1cb99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a7575c-6dee-416b-90f5-d06cba384815",
   "metadata": {
    "id": "53a7575c-6dee-416b-90f5-d06cba384815"
   },
   "source": [
    "## Part 1: Universal Aproximators\n",
    "Artificial Neural Networks (ANN) can be seen as a functions aproximators, optimized on provided data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76626b3-ac6f-40e4-b855-b27567a6417f",
   "metadata": {
    "id": "f76626b3-ac6f-40e4-b855-b27567a6417f"
   },
   "source": [
    "### Artificial Neuron Model\n",
    "* It is slightly modified __McCulloch-Pitts Neuron (1943)__. \n",
    "    * instead of Mculloch-Pitts model, you can use any activation function, not just the original  proposition of **step function** with configurable threshold. \n",
    "* It would not be a huge mistake to call it colloquially as \"**McCulloch-Pitts neuron model**\" or just \"**neuron model**\". \n",
    "* Also known as **Sigma Neuron** (due to the sigma symbol for summation).\n",
    "* It literaty works in 3 steps:\n",
    "    1) Multiply **inputs** by their corresponding **weights**,\n",
    "    2) **Add all products with** an offset term called **bias**,\n",
    "    3) Pass the sum result through a \"cramping\" function called **activation function**.\n",
    "        * For instance, __sigmoid(x)__ maps input to range (0,1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adbbe03-de7f-40f8-90da-bfb4a9cb4c5a",
   "metadata": {
    "id": "8adbbe03-de7f-40f8-90da-bfb4a9cb4c5a"
   },
   "source": [
    "### Matrix representation\n",
    "Instead of writing equations for multiple neurons, we can arrange multiplication and summation operations into matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481de559-704b-41b1-9bc5-56cfe264fa7f",
   "metadata": {
    "id": "481de559-704b-41b1-9bc5-56cfe264fa7f",
    "tags": []
   },
   "source": [
    "### Example activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e823ccd-58d0-47bf-9719-1147c0a8f744",
   "metadata": {},
   "source": [
    "### Example (deep) artificial neural network\n",
    "Talking about \"shallow\" or \"deep\" ANN is tricky - it depends mainly on the context. More than 2-3 layers can be considered as \"deep\" network.\n",
    "\n",
    "As you can wonder, the _Deep Learning_ subject is focused on ANNs with multiple layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wJEnxqkxxkmP",
   "metadata": {
    "id": "wJEnxqkxxkmP"
   },
   "source": [
    "### Forward propagation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93b1359-e26a-4f04-9627-9c761743992f",
   "metadata": {
    "id": "c93b1359-e26a-4f04-9627-9c761743992f",
    "outputId": "dd0fd2b0-cd26-4548-9bc2-298ecb16839b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Layer size\n",
    "input_size = 2\n",
    "neuron_units = 10\n",
    "\n",
    "# Initialization\n",
    "x = np.random.randn(input_size, 1)\n",
    "weights = np.random.randn(neuron_units, input_size)\n",
    "biases = np.random.randn(neuron_units, 1)\n",
    "\n",
    "# Forward propagation\n",
    "activation = np.dot(weights, x) + biases\n",
    "y = ReLU(activation)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CvBemWp_ybda",
   "metadata": {
    "id": "CvBemWp_ybda"
   },
   "source": [
    "### Gradient descent\n",
    "* **Gradient Descent** is a method of optimizing (training) ANN's weights and biases.\n",
    "    * There are other methods (ex. The Hebb's Rule) to train ANNs, but they are don't work as good as Gradient Descent\n",
    "* It consist of 3 main steps:\n",
    "    * **Forward Propagation** - to calculate output for comparison with desired output (i.e. to form a optimization problem with a Cost function).\n",
    "    * **Backward Propagation** - to calculate derivatives of weights & biases (in respect to some Cost function).\n",
    "    * **Update Parameters** - to (slightly) adjust weights & biases according to calculated derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2483f130-79c9-4647-a882-44ce1744301c",
   "metadata": {},
   "source": [
    "#### Passing gradient to previous layer\n",
    "* The **Cost function** $C$ is calculated **only once** for the output of the **whole network**.\n",
    "    * NOTE: the cost function is also known as **loss function**.\n",
    "* In general calculating $$\\dfrac{\\partial C}{\\partial a^{(L-1)}}$$ will result in a vector of derivatives of all activations from previous layer.\n",
    "* To calculate updates of weights & biases in previous layer $(L-1)$, it is necessary to use each element from the derivative vector $\\dfrac{\\partial C}{\\partial a^{(L-1)}}$ for corersponding activations of neurons $a^{(L-1)}_1, a^{(L-1)}_2, \\dots, a^{(L-1)}_m$ where $m$ is the size of $(L-1)$ layer.\n",
    "* Then, just apply the chain rule further, adding more multiplication terms (like derivative of the activation function in the current layer).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fbdcd7-cedf-4159-9811-d81bb926e8e3",
   "metadata": {},
   "source": [
    "#### Parameters update\n",
    "```python\n",
    "weights = weights - alpha * weights_derivative\n",
    "bias = bias - alpha * bias_derivative\n",
    "```\n",
    "where `alpha` is a **learning rate** parameter which helps us not overshot the local minima during the optimization process of the weights & biases.\n",
    "\n",
    "We are **subtracting** the calculated gradient (i.e. derivative) to descent into minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l66ypF3Um9Bs",
   "metadata": {
    "id": "l66ypF3Um9Bs"
   },
   "source": [
    "### Gradient Descent implementation example\n",
    "With:\n",
    "* Forawrd Propagation\n",
    "* Backward Propagation \n",
    "* Update Params\n",
    "\n",
    "and definitions of 2 activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_X7hSq0qm7rU",
   "metadata": {
    "id": "_X7hSq0qm7rU"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "GOAL: Build a ANN network with 2 layers\n",
    "* 1 layer:\n",
    "    * Input size: 10\n",
    "    * Output size, (aka. Neuron units; aka. layer size): 10\n",
    "    * Activation function (for all units): ReLU\n",
    "* 2 layer:\n",
    "    * Input size: (same as previous layer size, here: 10)\n",
    "    * Output size: 2\n",
    "    * Activation function: Softmax\n",
    "'''\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from collections import namedtuple\n",
    "\n",
    "# =============================== #\n",
    "\n",
    "# Let's start with definitions of some activation functions\n",
    "# ReLU with derivative for the 1st layer\n",
    "\n",
    "def ReLU(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def ReLU_deriv(Z):\n",
    "    return Z > 0\n",
    "\n",
    "# And Softmax for the 2nd layer \n",
    "# taken from https://stackoverflow.com/a/54977170\n",
    "\n",
    "def softmax(Z):\n",
    "    e = np.exp(Z)\n",
    "    return e / np.sum(e, axis=1)\n",
    "\n",
    "def softmax_deriv(Z):\n",
    "    # Reshape the 1-d softmax to 2-d so that np.dot will do the matrix multiplication\n",
    "    Z_reshaped = Z.reshape(-1,1)\n",
    "    return np.diagflat(Z_reshaped) - np.dot(Z_reshaped, Z_reshaped.T)\n",
    "\n",
    "# =============================== #\n",
    "\n",
    "# Now, we need to somehow randomly initizalize weights & biases.\n",
    "# Let's start with some preoparation, and define namedtuples for passing our network parameters\n",
    "Layer = namedtuple('Layer', ['weights', 'bias'])\n",
    "ANN = namedtuple('ANN', ['layer_1', 'layer_2'])\n",
    "\n",
    "# Then, for simplicity, let's initialize our network in naive way\n",
    "def init_ann_params(input_size: int, layer_1_size: int, layer_2_size: int) -> ANN:\n",
    "    \n",
    "    # The weights matrix must have dimensions equal to sizes of the input and the number of neural units\n",
    "    weights_1 = np.random.rand(layer_1_size, input_size) - 0.5\n",
    "    # The bias vector will have a one parameter (the bias) for each neural unit\n",
    "    bias_1 = np.random.rand(layer_1_size, 1)- 0.5\n",
    "    # Let's pack it into our namedtuple\n",
    "    layer_1 = Layer(weights=weights_1, bias=bias_1)\n",
    "    \n",
    "    # Let's do it analogously with 2nd layer.\n",
    "    # The input size must mach the size of the previous layer.\n",
    "    # The output size of this layer will be also the output size of the whole network.\n",
    "    weights_2 = np.random.rand(layer_2_size, layer_1_size)- 0.5\n",
    "    bias_2 = np.random.rand(layer_2_size, 1)- 0.5\n",
    "    layer_2 = Layer(weights=weights_2, bias=bias_2)\n",
    "    \n",
    "    # Finally, return our fresh network, packed in our namedtuple\n",
    "    return ANN(layer_1, layer_2)\n",
    "\n",
    "# =============================== #\n",
    "\n",
    "# It's time for Forward Propagation definition\n",
    "def forward_prop(ann: ANN, input_x: np.array) -> Tuple[np.array, np.array, np.array, np.array]:\n",
    "    \n",
    "    # Retrieve parameters for layer 1\n",
    "    layer_1 = ann.layer_1\n",
    "    # Using matrix notation multiply weights matrix by input vector and add bias vector for whole layer 1 \n",
    "    sum_1 = layer_1.weights.dot( input_x ) + layer_1.bias\n",
    "    # Then use the sum vector (each element consist of sum for each neuron in this layer) as input for activation function\n",
    "    activate_1 = ReLU(sum_1)\n",
    "    \n",
    "    # Moving forward to the 2nd layer - retrieve parameters\n",
    "    layer_2 = ann.layer_2\n",
    "    # Use the output of the 1st layer as input for the matrix calcucations in 2nd layer\n",
    "    sum_2 = layer_2.weights.dot( input_x ) + layer_2.bias\n",
    "    # Use the sum vector with the activation function of the last layer\n",
    "    activate_2 = softmax(sum_2)\n",
    "    \n",
    "    # Return the sums and the activations results\n",
    "    # (The sums will be needed to calculate derivatives of the activation functions!)\n",
    "    return sum_1, activate_1, sum_2, activate_2\n",
    "\n",
    "# =============================== #\n",
    "# After the forward propagation it's time for\n",
    "# The creme de la creme of the implementation - Backward Propagation \n",
    "\n",
    "# For the input we will need:\n",
    "#  1) sums and activations from the forward propagation,\n",
    "#  2) the weights of the all previous layers\n",
    "#  3) input sample with corresponding output sample (just one pair of samples!)\n",
    "def backward_prop(sum_1: np.array, \n",
    "                  activate_1: np.array, \n",
    "                  sum_2: np.array, \n",
    "                  activate_2: np.array,\n",
    "                  ann: ANN,\n",
    "                  input_x: np.array, \n",
    "                  output_y: np.array) -> Tuple[np.array, np.array, np.array, np.array]:\n",
    "    \n",
    "    # First, calculate the derivative of the cost function\n",
    "    # (Remember - it will always return a scalar value! )\n",
    "    cost_deriv = 2 * np.sum(activate_2 - output_y)\n",
    "    \n",
    "    # Let's use it to calculate updates (derivatives) of the weights & biases in the last layer\n",
    "    \n",
    "    # Studying the equations of the backprog, we can observe, that the bias term is slightly easier to implement\n",
    "    # What is more, *we can reuse it* in calculations of weights and the layer's input derivatives!\n",
    "    # NOTE: The result is a vector!\n",
    "    bias_2_deriv = softmax_deriv(sum_2) * cost_deriv\n",
    "    \n",
    "    # In addition, to obtain the derivative of the weights we need just to multiply the bias deriv by the layer's input!\n",
    "    # NOTE: Using 2 vectors we want to obtain result with dimensions of weights matrix!\n",
    "    weights_2_deriv = activate_1.dot( bias_2_deriv.transpose() )\n",
    "    \n",
    "    # Now, lest propagate the gradient to the first layer by calculating the derivative of the cost function in terms of the input\n",
    "    layer_2 = ann.layer_2\n",
    "    activate_1_deriv = layer_2.weights.dot( bias_2_deriv )\n",
    "    \n",
    "    # We can now calculate the derivatives for the 1st layer, reusing the above logic\n",
    "    bias_1_deriv = ReLU_deriv(sum_1) * activate_1_deriv\n",
    "    weights_1_deriv = input_x.dot( bias_1_deriv.transpose() )\n",
    "    # NOTE: We don't want to calculate the derivative of the input of the wole network\n",
    "    # (i.e. we don't want to pass the gradient to the dataset)\n",
    "    \n",
    "    # Return calculated updates (derivaites) for the weights and biases\n",
    "    return weights_1_deriv, bias_1_deriv, weights_2_deriv, bias_2_deriv\n",
    "\n",
    "\n",
    "# =============================== #\n",
    "\n",
    "# Now lets define function for updating network parameters - weights & biases\n",
    "# The alpha paramaeters is a learning rate\n",
    "# Let's update the params in naive way for further simplification\n",
    "def update_params(ann: ANN, \n",
    "                  weights_1_deriv: np.array, \n",
    "                  bias_1_deriv: np.array, \n",
    "                  weights_2_deriv: np.array, \n",
    "                  bias_2_deriv: np.array, \n",
    "                  alpha: float) -> ANN:\n",
    "    layer_1 = ann.layer_1\n",
    "    layer_1.weights = layer_1.weights - alpha * weights_1_deriv\n",
    "    layer_1.bias = layer_1.bias - alpha * bias_1_deriv\n",
    "    \n",
    "    layer_2 = ann.layer_2\n",
    "    layer_2.weights = layer_2.weights - alpha * weights_2_deriv\n",
    "    layer_2.bias = layer_2.bias - alpha * bias_2_deriv\n",
    "    \n",
    "    # Return the ANN with updated weightes & biases\n",
    "    return ANN(layer_1, layer_2)\n",
    "\n",
    "# =============================== #\n",
    "\n",
    "# Finally, let's define Gradient Descent to call above functions\n",
    "def gradient_descent(input_x: np.array,\n",
    "                     output_y: np.array, \n",
    "                     alpha: float, \n",
    "                     iterations: int) -> ANN:\n",
    "    \n",
    "    # First, initialize network with random parameters\n",
    "    ann = init_ann_params()\n",
    "    for _ in range(iterations):\n",
    "        # 1) Forward Propragation\n",
    "        sum_1, activate_1, sum_2, activate_2 = forward_prop(ann, input_x)\n",
    "        # 2) Backwards Propagation\n",
    "        weights_1_deriv, bias_1_deriv, weights_2_deriv, bias_2_deriv = backward_prop(sum_1,\n",
    "                                                                                     activate_1, \n",
    "                                                                                     sum_2, \n",
    "                                                                                     activate_2, \n",
    "                                                                                     ann, \n",
    "                                                                                     input_x, \n",
    "                                                                                     output_y)\n",
    "        # 3) Update parameters\n",
    "        ann = update_params(ann, \n",
    "                            weights_1_deriv, \n",
    "                            bias_1_deriv, \n",
    "                            weights_2_deriv, \n",
    "                            bias_2_deriv,\n",
    "                            alpha)\n",
    "        \n",
    "    # Return the updated network\n",
    "    return ann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841e1449-3f11-43f5-afc7-26af0c6b834e",
   "metadata": {
    "id": "841e1449-3f11-43f5-afc7-26af0c6b834e"
   },
   "source": [
    "## Part 2: Basic implementation\n",
    "In the following sections we will depend mainly on Numpy package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e035cd9-e4f1-4a72-b01d-00ab4e5c5b67",
   "metadata": {
    "id": "5e035cd9-e4f1-4a72-b01d-00ab4e5c5b67",
    "outputId": "801beb0f-1be3-4642-e33b-4f467475a40e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bd9ab2-49bd-4f7c-8d89-b5ed7a88db8c",
   "metadata": {
    "id": "77bd9ab2-49bd-4f7c-8d89-b5ed7a88db8c"
   },
   "source": [
    "### Task 2.1\n",
    "Implement an artificial neuron class, with **sigmoid** activation function. Use matrix operations (from Numpy package). Remember to define the activation function derivative. You can use following class-template or implement whole class by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99a8b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Tuple\n",
    "\n",
    "def activation_function(x: float) -> float:\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def activation_function_deriv(x: float) -> float:\n",
    "    sigmoid = activation_function(x)\n",
    "    return sigmoid * (1 - sigmoid)\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, input_size: int, act_func: Callable, act_func_deriv: Callable):\n",
    "        self._init_weights_and_bias(input_size)\n",
    "        self._activation_function = act_func\n",
    "        self._activation_function_deriv = act_func_deriv\n",
    "    \n",
    "    def _init_weights_and_bias(self, input_size: int):\n",
    "        self.weights = np.random.randn(input_size) - 0.5\n",
    "        self.bias = np.random.randn() - 0.5\n",
    "    \n",
    "    def __call__(self, x: np.array) -> float:\n",
    "        return self._forward_propagation(x)\n",
    "    \n",
    "    def _forward_propagation(self, x: np.array) -> float:\n",
    "        z = np.dot(self.weights, x) + self.bias\n",
    "        return self._activation_function(z)\n",
    "    \n",
    "    def gradient_descent(self, x: np.array, y_target: float, alpha: float, iterations: int) -> None:\n",
    "        self.alpha = alpha\n",
    "        for _ in range(iterations):\n",
    "            self.weights_deriv, self.bias_deriv = self._backward_propagation(x, y_target)\n",
    "            self._update_weights_and_bias()\n",
    "    \n",
    "    def _backward_propagation(self, x: np.array, y: float) -> Tuple[np.array, float]:\n",
    "        # 1st - full forward propagation\n",
    "        z = np.dot(self.weights, x) + self.bias\n",
    "        a = self._activation_function(z)\n",
    "        \n",
    "        # 2nd - calculate the loss function (error)\n",
    "        \n",
    "        # 3rd - calculate derivatives based on forward propagation and the error\n",
    "        dz = 2 * (a - y) * self._activation_function_deriv(z)\n",
    "        dw = dz * x\n",
    "        db = dz\n",
    "        \n",
    "        return dw, db\n",
    "    \n",
    "    def _update_weights_and_bias(self):\n",
    "        self.weights = self.weights - self.alpha * self.weights_deriv\n",
    "        self.bias = self.bias - self.alpha * self.bias_deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7c77a2-2691-48fd-8bf5-7b90f4eabfa4",
   "metadata": {
    "id": "8e7c77a2-2691-48fd-8bf5-7b90f4eabfa4"
   },
   "source": [
    "## Part 3: Artificial Neuron as binary clasifier\n",
    "A single neuron used as binary classifier is also known as *perceptron*, frequently used as building block for *dense* layer. It can be used for logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2f52f7-dcba-41d2-8a51-6b7415ba0e48",
   "metadata": {
    "id": "3b2f52f7-dcba-41d2-8a51-6b7415ba0e48",
    "tags": []
   },
   "source": [
    "### Task 3.1\n",
    "1) Using your Neuron class construct a following ANN:\n",
    "  * Input size: 2\n",
    "  * 1 layer with 1 unit with any activation function\n",
    "  * Output size: 1\n",
    "\n",
    "2) Perform separate trainings on provided datasets of truth tables of logic gates. You can experiment with number of iterations (start with n=500) and learnining rate (start with alpha = 0.1)\n",
    "\n",
    "3) Visualize each dataset and ANN's result (a regression line, as function of two inputs).\n",
    "\n",
    "4) Comment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b277456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13168fe9-18ef-4c9e-ac57-2cd9f86f91ba",
   "metadata": {
    "id": "13168fe9-18ef-4c9e-ac57-2cd9f86f91ba"
   },
   "source": [
    "#### OR gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ee6a1c-f9d3-40b1-9708-9fa11f2d0699",
   "metadata": {
    "id": "53ee6a1c-f9d3-40b1-9708-9fa11f2d0699"
   },
   "outputs": [],
   "source": [
    "dataset_or_x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).astype('float64')\n",
    "dataset_or_y = [0, 1, 1, 1]\n",
    "\n",
    "neuron = Neuron(\n",
    "    input_size=2,\n",
    "    act_func=activation_function,\n",
    "    act_func_deriv=activation_function_deriv\n",
    ")\n",
    "\n",
    "alpha = 0.1\n",
    "iterations = 500\n",
    "\n",
    "for i in range(iterations):\n",
    "    for x, y in zip(dataset_or_x, dataset_or_y):\n",
    "        neuron.gradient_descent(x, y, alpha, 1)\n",
    "\n",
    "predictions = [neuron(x) for x in dataset_or_x]\n",
    "\n",
    "for x, prediction in zip(dataset_or_x, predictions):\n",
    "    print(f\"Input: {x}, Predicted Output: {prediction:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142f7b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = -0.5, 1.5\n",
    "y_min, y_max = -0.5, 1.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                     np.arange(y_min, y_max, 0.01))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "Z = np.array([neuron(x) for x in grid])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.contourf(xx, yy, Z, levels=[0, 0.5, 1], alpha=0.3, colors=['blue', 'orange'])\n",
    "plt.scatter(dataset_or_x[:, 0], dataset_or_x[:, 1], c=dataset_or_y, edgecolor='k', s=100, marker='o', label='data points')\n",
    "plt.title('OR gate: ANN predictions and decision boundary')\n",
    "plt.xlabel('input 1')\n",
    "plt.ylabel('input 2')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.axhline(0, color='black',linewidth=0.5, ls='--')\n",
    "plt.axvline(0, color='black',linewidth=0.5, ls='--')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9228ee60-ac7b-405d-96e7-70798343b7c5",
   "metadata": {
    "id": "9228ee60-ac7b-405d-96e7-70798343b7c5"
   },
   "source": [
    "#### AND gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807a5c29-1b3e-4f1c-9531-386031d23d35",
   "metadata": {
    "id": "807a5c29-1b3e-4f1c-9531-386031d23d35"
   },
   "outputs": [],
   "source": [
    "dataset_and_x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).astype('float64')\n",
    "dataset_and_y = [0, 0, 0, 1]\n",
    "\n",
    "neuron = Neuron(\n",
    "    input_size=2,\n",
    "    act_func=activation_function,\n",
    "    act_func_deriv=activation_function_deriv\n",
    ")\n",
    "\n",
    "alpha = 0.01\n",
    "iterations = 10000\n",
    "\n",
    "for i in range(iterations):\n",
    "    for x, y in zip(dataset_and_x, dataset_and_y):\n",
    "        neuron.gradient_descent(x, y, alpha, 1)\n",
    "\n",
    "predictions = [neuron(x) for x in dataset_and_x]\n",
    "\n",
    "for x, prediction in zip(dataset_and_x, predictions):\n",
    "    print(f\"Input: {x}, Predicted Output: {prediction:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5c3889",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = -0.5, 1.5\n",
    "y_min, y_max = -0.5, 1.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                     np.arange(y_min, y_max, 0.01))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "Z = np.array([neuron(x) for x in grid])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.contourf(xx, yy, Z, levels=[0, 0.5, 1], alpha=0.3, colors=['blue', 'orange'])\n",
    "plt.scatter(dataset_and_x[:, 0], dataset_and_x[:, 1], c=dataset_and_y, edgecolor='k', s=100, marker='o', label='data points')\n",
    "plt.title('AND gate: ANN predictions and decision boundary')\n",
    "plt.xlabel('input 1')\n",
    "plt.ylabel('input 2')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.axhline(0, color='black',linewidth=0.5, ls='--')\n",
    "plt.axvline(0, color='black',linewidth=0.5, ls='--')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34529eb2-79db-4f9a-ba4c-8f4d975234be",
   "metadata": {
    "id": "34529eb2-79db-4f9a-ba4c-8f4d975234be",
    "tags": []
   },
   "source": [
    "#### XOR gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66c8471-0404-436f-9989-04d1140fd33f",
   "metadata": {
    "id": "d66c8471-0404-436f-9989-04d1140fd33f"
   },
   "outputs": [],
   "source": [
    "dataset_xor_x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).astype('float64')\n",
    "dataset_xor_y = [0, 1, 1, 0]\n",
    "\n",
    "neuron = Neuron(\n",
    "    input_size=2,\n",
    "    act_func=activation_function,\n",
    "    act_func_deriv=activation_function_deriv\n",
    ")\n",
    "\n",
    "alpha = 0.001\n",
    "iterations = 1000\n",
    "\n",
    "for i in range(iterations):\n",
    "    for x, y in zip(dataset_xor_x, dataset_xor_y):\n",
    "        neuron.gradient_descent(x, y, alpha, 1)\n",
    "\n",
    "predictions = [neuron(x) for x in dataset_xor_x]\n",
    "\n",
    "for x, prediction in zip(dataset_xor_x, predictions):\n",
    "    print(f\"Input: {x}, Predicted Output: {prediction:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6faf808-9db9-4088-8333-e3e4588d8f6f",
   "metadata": {
    "id": "e6faf808-9db9-4088-8333-e3e4588d8f6f"
   },
   "source": [
    "## Part 4: Multilayer perceptron\n",
    "More neurons can be stacked together to model nonlinear properties.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6292ecb7-b6c0-42ff-a7c7-83cee0b5a2cb",
   "metadata": {
    "id": "6292ecb7-b6c0-42ff-a7c7-83cee0b5a2cb"
   },
   "source": [
    "### Task 4.1\n",
    "In this task you have to implement following ANN:\n",
    "* Input size: 2\n",
    "* 1 layer with 2 units with sigmoid activation function\n",
    "* 1 layer with 1 unit with sigmoid activation function\n",
    "* Output size: 1\n",
    "    \n",
    "Your Neuron class was not designed for ambitious merging of weights and biases during the gradient descent, nor for passing outputs to perform forward propagation. To overcome such inconvenience, please manually define dataflow and method calling for all Neurons. You can expand provided example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943d6f93-6749-40dc-a45c-d9f7168e7e94",
   "metadata": {
    "id": "943d6f93-6749-40dc-a45c-d9f7168e7e94"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, input_size: int, act_func: Callable, act_func_deriv: Callable, alpha: 0.1):\n",
    "        # TODO\n",
    "        self._neuron_1 = Neuron(input_size, act_func, act_func_deriv)\n",
    "        self._neuron_2 = Neuron(input_size, act_func, act_func_deriv)\n",
    "        self._neuron_3 = Neuron(input_size, act_func, act_func_deriv)\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def __call__(self, x: np.array) -> float:\n",
    "        return self._network_forward_propagation(x)\n",
    "\n",
    "    def _network_forward_propagation(self, x: np.array) -> float:\n",
    "        a1 = self._neuron_1(x)\n",
    "        a2 = self._neuron_2(x)\n",
    "        input_3 = np.array([a1, a2])\n",
    "        return self._neuron_3(input_3)\n",
    "\n",
    "    def _network_backwards_propagation(self, x: np.array, y: np.array) -> None:\n",
    "        # TODO\n",
    "        # 1st - full forward propagation\n",
    "        z1 = np.dot(self._neuron_1.weights, x) + self._neuron_1.bias\n",
    "        a1 = self._neuron_1._activation_function(z1)\n",
    "\n",
    "        z2 = np.dot(self._neuron_2.weights, x) + self._neuron_2.bias\n",
    "        a2 = self._neuron_2._activation_function(z2)\n",
    "\n",
    "        input_3 = np.array([a1, a2])\n",
    "        \n",
    "        z3 = np.dot(self._neuron_3.weights, input_3) + self._neuron_3.bias\n",
    "        a3 = self._neuron_3._activation_function(z3)\n",
    "\n",
    "        # 2nd - backward propagation\n",
    "\n",
    "        error_3_deriv = 2 * (a3 - y) # deriv of MSE\n",
    "        \n",
    "        delta_3 = error_3_deriv * self._neuron_3._activation_function_deriv(z3)\n",
    "\n",
    "        delta_2 = np.dot(delta_3, self._neuron_3.weights[1]) * self._neuron_2._activation_function_deriv(z2)\n",
    "\n",
    "        delta_1 = np.dot(delta_3, self._neuron_3.weights[0]) * self._neuron_1._activation_function_deriv(z1)\n",
    "\n",
    "        # 3rd calculate gradients\n",
    "        dw3, db3 = delta_3 * input_3, delta_3\n",
    "        dw2, db2 = delta_2 * x, delta_2\n",
    "        dw1, db1 = delta_1 * x, delta_1\n",
    "\n",
    "        return dw3, db3, dw2, db2, dw1, db1\n",
    "\n",
    "    def gradient_descent(self, x: np.array, y: np.array) -> None:\n",
    "        # TODO\n",
    "        gradient_sums = {'dw3': 0, 'db3': 0, 'dw2': 0, 'db2': 0, 'dw1': 0, 'db1': 0}\n",
    "        for x_i, y_i in zip(x, y):\n",
    "            dw3, db3, dw2, db2, dw1, db1 = self._network_backwards_propagation(x_i, y_i)\n",
    "            gradient_sums['dw3'] += dw3\n",
    "            gradient_sums['db3'] += db3\n",
    "            gradient_sums['dw2'] += dw2\n",
    "            gradient_sums['db2'] += db2\n",
    "            gradient_sums['dw1'] += dw1\n",
    "            gradient_sums['db1'] += db1\n",
    "        \n",
    "        m = len(x)\n",
    "        avg_gradients = {k: v / m for k, v in gradient_sums.items()}\n",
    "\n",
    "        self._neuron_3.weights -= self.alpha * avg_gradients['dw3']\n",
    "        self._neuron_3.bias -= self.alpha * avg_gradients['db3']\n",
    "\n",
    "        self._neuron_2.weights -= self.alpha * avg_gradients['dw2']\n",
    "        self._neuron_2.bias -= self.alpha * avg_gradients['db2']\n",
    "\n",
    "        self._neuron_1.weights -= self.alpha * avg_gradients['dw1']\n",
    "        self._neuron_1.bias -= self.alpha * avg_gradients['db1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dac784-9ba3-46ea-ac19-ea920fe34b64",
   "metadata": {
    "id": "e9dac784-9ba3-46ea-ac19-ea920fe34b64"
   },
   "source": [
    "### Task 4.2\n",
    "1) Train your ANN created in task 4.1 on the XOR dataset. You can experiment with number of iterations (start with n=500) and learning rate (start with alpha=0.1).\n",
    "\n",
    "2) Visualize the dataset and ANN's result (a regression line, as function of two inputs).\n",
    "\n",
    "3) Comment results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b268917-d473-4102-b6e1-2669475aeb6b",
   "metadata": {
    "id": "4b268917-d473-4102-b6e1-2669475aeb6b"
   },
   "outputs": [],
   "source": [
    "dataset_xor_x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).astype('float64')\n",
    "dataset_xor_y = [0, 1, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fdae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0d80de",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "iterations = 10000\n",
    "nn = NeuralNetwork(input_size=2, act_func=activation_function, act_func_deriv=activation_function_deriv, alpha=alpha)\n",
    "\n",
    "for _ in tqdm(range(iterations)):\n",
    "    nn.gradient_descent(dataset_xor_x, dataset_xor_y)\n",
    "\n",
    "\n",
    "predictions = [nn(x) for x in dataset_xor_x]\n",
    "\n",
    "for x, prediction in zip(dataset_xor_x, predictions):\n",
    "    print(f\"Input: {x}, Predicted Output: {prediction:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850e2fa0-d71b-42b5-9d68-25039e536e68",
   "metadata": {
    "id": "850e2fa0-d71b-42b5-9d68-25039e536e68"
   },
   "source": [
    "## Remarks\n",
    "* **Do not implement ANN by yourself** - use already tested open-source frameworks with hardware acceleration, such as PyTorch, Keras, TensorFlow, Jax+Haiku, etc.\n",
    "* **Every ANN is just an aproximator for a certain (often unknown) function** - Nothing more, nothing less. The learning procedure is data-based brutal force function derivation.\n",
    "* **Despite current knowledge, selecting ANN dimensions is still more art than science** - every fixed parameter can be considered as \"hyperparameter\", which can be further optimized by an adequate algorithm.\n",
    "* **Selecting an activation function is not trivial** - always consider dimishing gradient and calculation cost\n",
    "\n",
    "## Further reading\n",
    "* [YouTube - 3Blue1Brown - Backpropagation calculus](https://www.youtube.com/watch?v=tIeHLnjs5U8)\n",
    "* [Brilliant - Perceptron](https://brilliant.org/wiki/perceptron/)\n",
    "* [builtin - How Does Backpropagation in a Neural Network Work?](https://builtin.com/machine-learning/backpropagation-neural-network)\n",
    "* [edureka! - Backpropagation – Algorithm For Training A Neural Network](https://www.edureka.co/blog/backpropagation/)\n",
    "* [Visual backpropagation description](https://sebastianraschka.com/faq/docs/visual-backpropagation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e5d630",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "f76626b3-ac6f-40e4-b855-b27567a6417f",
    "8adbbe03-de7f-40f8-90da-bfb4a9cb4c5a",
    "481de559-704b-41b1-9bc5-56cfe264fa7f",
    "CvBemWp_ybda",
    "l66ypF3Um9Bs"
   ],
   "name": "ml_ann_introduction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml-task",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
